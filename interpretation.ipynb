{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azY_1lioaktL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68783aec-18c0-4bca-d859-dac57111e668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZDXt7Gbi08b",
        "outputId": "4cff09b6-fffd-4201-c6d2-c2174cd049a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (10.4.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.4 pypdfium2-4.30.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_NK49H6mnYK",
        "outputId": "18338a5e-f05f-4da2-e804-4f33b75e0113"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4auiRvLBi1jT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pdfplumber\n",
        "import re\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7F4wACWmHVe"
      },
      "outputs": [],
      "source": [
        "## Get the dataset ready"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uGcxgSwmHYP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bea97ea-cad5-40dd-d9f3-69da9bca8180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['text'],\n",
            "    num_rows: 62\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "import re\n",
        "\n",
        "# Path to your text file\n",
        "file_path = '/content/DI.txt'\n",
        "\n",
        "# Function to parse the text file\n",
        "def parse_text_file(file_path):\n",
        "    data = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "        # Split the content into sections\n",
        "        entries = content.split('\\n\\n')  # Assuming each entry is separated by a blank line\n",
        "\n",
        "        # Regex to find \"dream:\" and \"interpretation:\" fields\n",
        "        dream_pattern = r'dream:\\s*(.*)'\n",
        "        interpretation_pattern = r'interpretation:\\s*(.*)'\n",
        "\n",
        "        # Extract dream and interpretation pairs\n",
        "        for entry in entries:\n",
        "            dream_match = re.search(dream_pattern, entry, re.IGNORECASE)\n",
        "            interpretation_match = re.search(interpretation_pattern, entry, re.IGNORECASE)\n",
        "\n",
        "            # If both dream and interpretation are found, add them as a dictionary\n",
        "            if dream_match and interpretation_match:\n",
        "                dream = dream_match.group(1).strip()\n",
        "                interpretation = interpretation_match.group(1).strip()\n",
        "                data.append({\"dream\": dream, \"interpretation\": interpretation})\n",
        "\n",
        "    return data\n",
        "\n",
        "# Parse the file to get structured data\n",
        "parsed_data = parse_text_file(file_path)\n",
        "\n",
        "# Check the first few entries to verify parsing\n",
        "#print(parsed_data[:5])\n",
        "data_dict = {\n",
        "    \"text\": [f\"{entry['dream']} -> {entry['interpretation']}\" for entry in parsed_data],\n",
        "}\n",
        "\n",
        "# Convert list of dictionaries into a dictionary of lists\n",
        "\"\"\"data_dict = {\n",
        "    \"dream\": [entry[\"dream\"] for entry in parsed_data],\n",
        "    \"interpretation\": [entry[\"interpretation\"] for entry in parsed_data]\n",
        "}\"\"\"\n",
        "\n",
        "# Convert the dictionary of lists to a Hugging Face dataset\n",
        "dataset = Dataset.from_dict(data_dict)\n",
        "\n",
        "# View the dataset\n",
        "print(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7FoS9jjjkL7"
      },
      "outputs": [],
      "source": [
        "# The model that you want to train from the Hugging Face hub\n",
        "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# The instruction dataset to use\n",
        "#dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
        "\n",
        "# Fine-tuned model name\n",
        "new_model = \"Llama-2-7b-chat-finetune\"\n",
        "\n",
        "################################################################################\n",
        "# QLoRA parameters\n",
        "################################################################################\n",
        "\n",
        "# LoRA attention dimension\n",
        "lora_r = 64\n",
        "\n",
        "# Alpha parameter for LoRA scaling\n",
        "lora_alpha = 16\n",
        "\n",
        "# Dropout probability for LoRA layers\n",
        "lora_dropout = 0.1\n",
        "\n",
        "################################################################################\n",
        "# bitsandbytes parameters\n",
        "################################################################################\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False\n",
        "\n",
        "################################################################################\n",
        "# TrainingArguments parameters\n",
        "################################################################################\n",
        "\n",
        "# Output directory where the model predictions and checkpoints will be stored\n",
        "output_dir = \"./results\"\n",
        "\n",
        "# Number of training epochs\n",
        "num_train_epochs = 1\n",
        "\n",
        "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "\n",
        "# Batch size per GPU for training\n",
        "per_device_train_batch_size = 2\n",
        "\n",
        "# Batch size per GPU for evaluation\n",
        "per_device_eval_batch_size = 2\n",
        "\n",
        "# Number of update steps to accumulate the gradients for\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "gradient_checkpointing = True\n",
        "\n",
        "# Maximum gradient normal (gradient clipping)\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "# Initial learning rate (AdamW optimizer)\n",
        "learning_rate = 2e-4\n",
        "\n",
        "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
        "weight_decay = 0.001\n",
        "\n",
        "# Optimizer to use\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Learning rate schedule\n",
        "lr_scheduler_type = \"cosine\"\n",
        "\n",
        "# Number of training steps (overrides num_train_epochs)\n",
        "max_steps = -1\n",
        "\n",
        "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
        "warmup_ratio = 0.03\n",
        "\n",
        "# Group sequences into batches with same length\n",
        "# Saves memory and speeds up training considerably\n",
        "group_by_length = True\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 0\n",
        "\n",
        "# Log every X updates steps\n",
        "logging_steps = 25\n",
        "\n",
        "################################################################################\n",
        "# SFT parameters\n",
        "################################################################################\n",
        "\n",
        "# Maximum sequence length to use\n",
        "max_seq_length = None\n",
        "\n",
        "# Pack multiple short examples in the same input sequence to increase efficiency\n",
        "packing = False\n",
        "\n",
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 872,
          "referenced_widgets": [
            "de17cfe62a3545d2af4937982d771328",
            "7e756e6c1a7241eb8249871713e0fff2",
            "6ad983b31ccd4a228c1a68d9154c445e",
            "a44f507936b24b90b878df66c9555445",
            "98ce0d3a1c0a451d8f3aa6e75b5cf3e3",
            "eb31bca050d949dbadfc3d8cc20d5e29",
            "1e35f211774a40448f62c1029204958f",
            "05f40759dba04717b7868be18fd8dfc8",
            "a1e59d797b81487298bd88ee8a90723e",
            "5e322a9ecf8f4dad897492ba81383e18",
            "c158beac38e148718834854003c312cf",
            "cd6c87f40dce44209b160070b1a704c9",
            "7bcb146937fd4967b605c7beb594e1a2",
            "b1b5232c48a14c7091bc46654c2844be",
            "59208671cf534602a76c6e815775dd6e",
            "edfdc02b75db45aeab2a36d86a012fb5",
            "61c7d511107549468a35990fbf5723ce",
            "3cb8095c49f14876be10639ef84d35da",
            "4a284ec1d1ea49fab4e38d11a0c8751b",
            "e70d57995e814b4a8557f2a7d5f359d7",
            "7025ba6316f3475794ea18769bc34320",
            "5c534ec7904f4a7fbc157096f12b21e2",
            "3d49b50a619c42b189096e864e299af0",
            "ec6e73b36cd94724b713cf1d307a4e40",
            "58e693b000d847f781043f6b01d6b285",
            "dad03437e4994ee78599912eebe34248",
            "78c499ccbfd84ca48c62cd479db3c58d",
            "8c6f22cab0a44460a9231880abb0da0b",
            "959740e6c96b4d0fa81df7e8e8a8c15b",
            "20b53785146a4feda5306bc72f63b118",
            "8a404e8284464c1f9b6ac75374097753",
            "53fc8a6d86864bbd9f2a3bad29918601",
            "ddab097ac48d436b91ed41b76a879717",
            "76e47b202c69497d9341193a2947af55",
            "4af324455ed94232bb2184b156eef583",
            "27b2439e38ab449590e594225799d5fe",
            "6294dc32ad03465096a6631c7a141669",
            "1ee11e59874c4b21bf06dceadc7b5d5c",
            "7bcc992aa88345b0bcc0b6b8c2c4bbd5",
            "5abf9fcd680a4e24b16173cba3b329b6",
            "d0cdcbb8da274a09b8d710a1fe053141",
            "371499892edd4f41948992f6d38b6983",
            "f080b3c7f41643fd96523a65feb3c0ef",
            "9785f3ed041a4fa2a192c52265d1bb9b",
            "e972b104b6984166a4270f3f240213e4",
            "1953c905a287429aacc7b50ce6b299bb",
            "3a3dbe5e677741a5bc34835702c5bebc",
            "c50aa98a606a47aa9e7e0033e10d4edb",
            "5ff2d47dcb794fe186712ce9872146d0",
            "f61f5fcef75041db9920280fd431f933",
            "606efeff27b94b899211150d6e529121",
            "0618416a4b8b408c8cfa56513c1e53e6",
            "4c033515df6e471e8bd288241b0f43b4",
            "00b2320533834f379da02aea76db0233",
            "f90b4e20aef440ffabf3ee6845b9b2a9",
            "eef45e8a12a848e89232d162bc9179fb",
            "edd85daa574f4a35894680498c3fd71b",
            "544986bc081b42a5bc9e6870137cc9ba",
            "dd235139605e4c71a9c9e4b78954cdc1",
            "c52f785f2a1f4738b4fe9aa7fe003227",
            "2cee73081707429f99591fcd6e7a4ba3",
            "bba11cb3a48f4f2c97d195be4052de03",
            "90ac86654fd94c9ba2b048e581f5d9cf",
            "b24d2b29ba0f4f6a852c9df3dcd5813d",
            "876bd579005a4e66ac7345e023d59a88",
            "37c9feb580fd40b3b4e7ca586db59648",
            "35db8626276149aeabb9b5dcd3fb299d",
            "0e3e55dc0a7347139625d130e7232196",
            "ea585947c1ee4882b490f0a364632ad5",
            "50603c0c8c424d74b80e84a1b3e502c6",
            "205f09ae5aa549df9cb7998aa7f44375",
            "51e478db1d6a4b028c21b0edab23cd50",
            "1a58bf5a615f4d6cb79cfec0a9871f0d",
            "c532a6aa950b46ceb37bcbbd74be8696",
            "f07f808799f0483aaa7b62808ac8416b",
            "7e5675fe9d184911adfe4904ce7a120a",
            "de26a32c56214319b5c9f9d9ff870c52",
            "a404a40245de44bcaa25fd9363d5fec0",
            "6c8234355cbe4b21aa235b19ccdfe7d8",
            "d6444250bad04ad5b55d2abc50addacf",
            "01547ab4eef147f8b931910935268873",
            "e340bda0158849a6a3d3c6a5dd2b1323",
            "3aad91d60d924ae19f0db6d5b9655db8",
            "be813e2cfa6847d69188b87ff0af5855",
            "aae15b4e4db948308412a09a03da8fab",
            "60fdc46a0f4f40dbbe31b3ef68c7d9db",
            "b7bd3d58f24a423bbf29b73d0fe59816",
            "de8829b1273f44e6bcee717b61464e67",
            "6b809667d4374725a5fe881eb8820b20",
            "9a62631f01ab494c9f337cf6e3b4dc01",
            "8ac18bb6c09949c59ef66c6ffbee0604",
            "f2394a0c77b44d7f9c5e60a61b70d1d9",
            "8eb45a180b404cb18f7ca720714c8a4d",
            "7179083964414b5abeff6a394e62c5b9",
            "97e2b5fde63d4c208b940534c4848fc9",
            "890c2fa789894b9e8097b0a000a586dd",
            "cf7ba8ffea7a40058472652267c43d99",
            "ae4119e76ea54dac8f204bf8067d2282",
            "5f4a32607b6742abbdf54f0a5310500c",
            "d627e92ffc2345ec9cd4bcd4e8633b0f",
            "42ae538301f347c08cb28ea5d10839c0",
            "db4a6f4b9ded441dbdb517de7a547ebe",
            "b751db7c254b468f988935fb495b8836",
            "f4b2c00296b24f12a7c2224a00833b40",
            "ab1fd6a38de840a790ad4dee49d89c66",
            "df0f7f76d4854d9f88e7765b9dcbc7f4",
            "f172c6811e07497ebc0e2d281b9400ef",
            "73b440ccdae547a6b126c73e7e2b36b2",
            "224a946c9953418c9dbb0e8d87cd9308",
            "f01e6b5d229743bfbd38de4683751fef",
            "ce597e1113444aa78ba547f896675420",
            "09b101d9a4c84cce9ecb6cf12eb4dde2",
            "52035805a4ed493fa3e12e3b756d3250",
            "6d67e895709b44d28d882bfe2a57fb3a",
            "5a8a51914dae46fc82a7b8c8c8bb0649",
            "e14525a01e8843eea862de64091a460c",
            "619b1cd2c3294550bf3eb00674a325a3",
            "e5a9a2e39d4847c8a67626455608b3b9",
            "39d41695f39b4cb5b8712a9e9835a2e1",
            "611069f240c04b8b80379aa1c5a04aa3",
            "d2e752286acc450483f7d04832134628",
            "62980a8214994c3490ca38b3e1c719f5",
            "bd5ffbe1f6a143c5975a8a22edcee232",
            "54aad26dec28416bbf1ee1f2fd2dfdae",
            "9d87e26782d249749a82639956f9c3e8",
            "49085132e32c4df1bd27b19572244220",
            "cb6eb00fb726413cbdc44b7328d94450",
            "71bbbb7b57e14229a95a784d403e8f0b",
            "07eae293e78b487580d87e3d72da2409",
            "0c742e040ce54f39968cc5843a8eb55a",
            "1a4c398b5a9e4e1294a500f1936976e3",
            "8b4c067351d748bdac244285b264f9d6",
            "4bb086a42d674f5894d107b8d1f8b97c",
            "7a74a068b01743d1829acfc846c62673",
            "40b161436fc54904bf3ce2eaf6724786",
            "cc1ec9631b9243468dce1a4f4ce052c6",
            "19998b33b77c42989a9688cf2467c5b8",
            "6393e592f8a148d2a55a7dac1dd6456b",
            "cc89c5ca453f45cfa0fc19a672c83376",
            "f92dd9a65d6d45e59ca9ea22911f98c5",
            "981d9a1f10f2442596069fa6bd95ce02",
            "d8b84bfdf8e241bd998d7a4349fbff40",
            "2ffaae6e278c4ee9a1d9e952eec4683c"
          ]
        },
        "id": "lsU8C3LEn_n0",
        "outputId": "226d59f0-c1c6-4f9a-ca19-f8c4b4041e8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de17cfe62a3545d2af4937982d771328"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd6c87f40dce44209b160070b1a704c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d49b50a619c42b189096e864e299af0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76e47b202c69497d9341193a2947af55"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e972b104b6984166a4270f3f240213e4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eef45e8a12a848e89232d162bc9179fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35db8626276149aeabb9b5dcd3fb299d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a404a40245de44bcaa25fd9363d5fec0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b809667d4374725a5fe881eb8820b20"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d627e92ffc2345ec9cd4bcd4e8633b0f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce597e1113444aa78ba547f896675420"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62980a8214994c3490ca38b3e1c719f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/62 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4bb086a42d674f5894d107b8d1f8b97c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='31' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [31/31 01:01, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>2.485900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=31, training_loss=2.463759022374307, metrics={'train_runtime': 70.5224, 'train_samples_per_second': 0.879, 'train_steps_per_second': 0.44, 'total_flos': 334722399191040.0, 'train_loss': 2.463759022374307, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Load tokenizer and model with QLoRA configuration\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "# Load base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "# Load LLaMA tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
        "\n",
        "# Load LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"tensorboard\"\n",
        ")\n",
        "\n",
        "# Set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing,\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAchKIMhoT7Y"
      },
      "outputs": [],
      "source": [
        "trainer.model.save_pretrained(new_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "RlYV2CeOsqTa",
        "outputId": "6e111e44-5eac-450a-b1bd-557be034e7f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import torch\\ntorch.cuda.empty_cache()'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "\"\"\"import torch\n",
        "torch.cuda.empty_cache()\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTnCHoIzs1wf",
        "outputId": "08348a72-a911-46aa-8e44-c071270d810a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>[INST] dream: I was in a dark room. I was looking at the celing. Then i woke up. [/INST]  Dreams can be highly personal and subjective, and the meaning of your dream can vary depending on your own experiences and emotions. However, here are some possible interpretations of your dream:\n",
            "\n",
            "1. Fear of the unknown: The dark room could represent a fear of the unknown or a feeling of being overwhelmed by a situation. You may be feeling uncertain or uncomfortable about something in your waking life.\n",
            "2. Lack of control: The ceiling in your dream could represent a feeling of being out of control or powerless in your waking life. You may be feeling like you're stuck in a situation and can't do anything to change it.\n",
            "3. Inner reflection: The ceiling in your dream could also represent your own thoughts and emotions. You may be reflecting on your own inner world and trying to make sense of your feelings.\n",
            "4. Transition: Waking up from the dream could represent a transition or a new beginning in your waking life. You may be moving from one stage of life to another, or you may be experiencing a change in your perspective or outlook.\n",
            "5. Emotional state: The dream could simply be a reflection of your emotional state. You may be feeling anxious or stressed, and the dream is a manifestation of those feelings.\n",
            "\n",
            "It's important to remember that dreams are highly personal and can have multiple meanings depending on the individual. The best way to understand the meaning of your dream is to reflect on your own experiences and emotions and see how they may be connected to the elements in the dream.\n"
          ]
        }
      ],
      "source": [
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Run text generation pipeline with our next model\n",
        "prompt = \"\"\"dream: I was in a dark room. I was looking at the celing. Then i woke up.\"\"\"\n",
        "\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500)\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EC5EUhiTt9gg"
      },
      "outputs": [],
      "source": [
        "# Empty VRAM\n",
        "del model\n",
        "del pipe\n",
        "del trainer\n",
        "import gc\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOlc9heYxjKb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buVlgsp9xlFG"
      },
      "outputs": [],
      "source": []
    }
  ]
}